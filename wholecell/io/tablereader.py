
from __future__ import absolute_import, division, print_function

import os
import json

import numpy as np

from . import tablewriter as tw

__all__ = [
	"TableReader",
	]

class TableReaderError(Exception):
	"""
	Base exception class for TableReader-associated exceptions.
	"""
	pass


class VersionError(TableReaderError):
	"""
	An error raised when the input files claim to be from a different format or
	version of the file specification.
	"""
	pass


class DoesNotExistError(TableReaderError):
	"""
	An error raised when a column or attribute does not seem to exist.
	"""
	pass


class _ColumnHeader(object):
	'''A Column's HEADER info, decoded from the data file header bytes.'''
	def __init__(self, header_bytes):
		(magic_signature,
		self.bytes_per_entry,
		self.elements_per_entry,
		entries_per_block,
		compression_type,
		descr_json) = tw.HEADER.unpack(header_bytes)

		if magic_signature != tw.COLUMN_SIGNATURE:
			raise VersionError('Unrecognized Column file header')

		if compression_type != 0:
			raise VersionError('Unsupported Column compression type')

		if entries_per_block != 1:
			raise VersionError('Unsupported Column entry packing')

		descr = json.loads(descr_json)
		if isinstance(descr, basestring):
			self.dtype = str(descr)  # really the dtype.descr
		else:
			# numpy requires list-of-tuples-of-strings
			# TODO(jerry): Support triples?
			self.dtype = [(str(n), str(t)) for n, t in descr]


class TableReader(object):
	"""
	Reads output generated by TableWriter.

	Parameters:
		path (str): Path to the input location (a directory).

	See also
	--------
	wholecell.io.tablewriter.TableWriter
	"""

	def __init__(self, path):
		self._path = path

		# Read the table's attributes file
		attributes_filename = os.path.join(path, tw.FILE_ATTRIBUTES)
		try:
			with open(attributes_filename) as f:
				self._attributes = json.load(f)

		except IOError as e:
			raise VersionError(
				"Could not read a table's attributes file ({})."
				" Unzip all table files if needed.".format(attributes_filename), e)

		# Check if the table's version matches the expected version
		version = self._attributes['_version']
		if version != tw.VERSION:
			raise VersionError("Expected version {} but found version {}".format(
				tw.VERSION, version))

		# List the column names. Ignore the 'attributes.json' file.
		self._columnNames = {p for p in os.listdir(path) if '.json' not in p}
		self._columnHeaders = {}  # maps column name to _ColumnHeader


	def readAttribute(self, name):
		"""
		Return an attribute value.

		Parameters
		----------
		name : str
			The name of the attribute.

		Returns
		-------
		The attribute, JSON-deserialized from a string.

		"""

		if name not in self._attributes:
			raise DoesNotExistError("No such attribute: {}".format(name))
		return self._attributes[name]


	def readColumn(self, name, indices=None):
		"""
		Load a full column (all entries). Each (row x column) entry is a
		1-D NumPy array. This method can optionally read just a slice of all
		those arrays: their subcolumns at the given `indices`.

		Parameters:
			name (str): The name of the column.
			indices (ndarray): Numpy array of ints. The specific subcolumn
				indices to read from each entry. If None, reads in all data.
				If provided, can give a performance boost for columns with many
				entries.
				NOTE: The performance benefit might only be realized if the file
				is in the disk cache (i.e. the file has been recently read),
				which should typically be the case.

		Returns:
			ndarray: a 2-D NumPy array (row x subcolumn) OR squeezed into a
			1-D array if the entries written were scalars or 1-element arrays
			or there's only one row.

		Notes:
		If entry sizes varies, this method cannot be used.

		TODO (John): Consider using np.memmap to defer loading of the data
			until it is operated on.  Early work (see issue #221) suggests that
			this may lead to cryptic performance issues.
		"""

		if name not in self._columnNames:
			raise DoesNotExistError("No such column: {}".format(name))

		with open(os.path.join(self._path, name), 'rb') as dataFile:
			header = self._loadHeader(name, dataFile)

			dataFile.seek(0, os.SEEK_END)
			file_size = dataFile.tell()
			nEntries = (file_size - tw.HEADER.size) // header.bytes_per_entry

			dataFile.seek(tw.HEADER.size)

			if indices is None:
				return np.frombuffer(
					dataFile.read(), header.dtype
					).copy().reshape(nEntries, -1).squeeze()

			else:
				data = np.zeros((nEntries, len(indices)), header.dtype)

				for i in range(nEntries):
					data[i, :] = np.frombuffer(
						dataFile.read(header.bytes_per_entry), header.dtype
						)[indices]

				return data.squeeze()


	def _loadHeader(self, name):
	def _loadHeader(self, name, dataFile):
		"""
		Load the named column's _ColumnHeader from disk or cache.

		SIDE EFFECTS: Reads the header from the file if it isn't cached, in
		that case moving the file's read position.

		Parameters:
			name (str): The column name.
			dataFile (BinaryIO): The open column data file @ seek position 0.
		"""

		if name in self._columnHeaders:
			return self._columnHeaders[name]

		header_data = dataFile.read(tw.HEADER.size)
		header = _ColumnHeader(header_data)
		self._columnHeaders[name] = header
		return header


	def allAttributeNames(self):
		"""
		Returns a list of all attribute names including Table metadata.
		"""
		return self._attributes.keys()


	def attributeNames(self):
		"""
		Returns a list of ordinary (client-provided) attribute names.
		"""
		names = [k for k in self._attributes.keys() if not k.startswith('_')]
		return names


	def columnNames(self):
		"""
		Returns the names of all columns.
		"""
		return list(self._columnNames)


	def close(self):
		"""
		Does nothing.

		The TableReader keeps no files open, so this method does nothing.

		Notes
		-----
		TODO (John): Consider removing this method.  At the moment are usage is
			inconsistent, and gives the impression that it is actually
			beneficial or necessary.

		"""
		pass
