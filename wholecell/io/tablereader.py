
from __future__ import absolute_import, division, print_function

from chunk import Chunk
import os
import json
import numpy as np
import zlib

from wholecell.utils import filepath
from . import tablewriter as tw

__all__ = [
	"TableReader",
	]

SUPPORTED_COMPRESSION_TYPES = (tw.COMPRESSION_TYPE_NONE, tw.COMPRESSION_TYPE_ZLIB)


class TableReaderError(Exception):
	"""
	Base exception class for TableReader-associated exceptions.
	"""
	pass


class VersionError(TableReaderError):
	"""
	An error raised when the input files claim to be from a different format or
	version of the file specification.
	"""
	pass


class DoesNotExistError(TableReaderError):
	"""
	An error raised when a column or attribute does not seem to exist.
	"""
	pass


class _ColumnHeader(object):
	'''Column header info read from a Column file's first chunk.'''
	def __init__(self, chunk):
		if chunk.getname() != tw.COLUMN_CHUNK_TYPE:
			raise VersionError('Not a supported Column file format/version')

		header_struct = chunk.read(tw.COLUMN_STRUCT.size)
		(self.bytes_per_entry,
		self.elements_per_entry,
		self.entries_per_block,
		self.compression_type) = tw.COLUMN_STRUCT.unpack(header_struct)

		if self.compression_type not in SUPPORTED_COMPRESSION_TYPES:
			raise VersionError('Unsupported Column compression type {}'.format(
				self.compression_type))

		descr_json = chunk.read()
		descr = json.loads(descr_json)

		if isinstance(descr, basestring):
			self.dtype = str(descr)  # really the dtype.descr
		else:
			# numpy requires list-of-tuples-of-strings
			# TODO(jerry): Support triples?
			self.dtype = [(str(n), str(t)) for n, t in descr]


class TableReader(object):
	"""
	Reads output generated by TableWriter.

	Parameters:
		path (str): Path to the input location (a directory).

	See also
	--------
	wholecell.io.tablewriter.TableWriter
	"""

	def __init__(self, path):
		self._path = path

		# Read the table's attributes file
		attributes_filename = os.path.join(path, tw.FILE_ATTRIBUTES)
		try:
			self._attributes = filepath.read_json_file(attributes_filename)

		except IOError as e:
			raise VersionError(
				"Could not read a table's attributes file ({})."
				" Version 2 tables are not supported."  # they could be...
				" Unzip all table files if needed.".format(attributes_filename), e)

		# Check if the table's version matches the expected version
		version = self._attributes['_version']
		if version != tw.VERSION:
			raise VersionError("Expected version {} but found version {}".format(
				tw.VERSION, version))

		# List the column file names. Ignore the 'attributes.json' file.
		self._columnNames = {p for p in os.listdir(path) if '.json' not in p}


	def readAttribute(self, name):
		"""
		Return an attribute value.

		Parameters:
			name (str): The attribute name.

		Returns
		-------
		The attribute value, JSON-deserialized from a string.
		"""

		if name not in self._attributes:
			raise DoesNotExistError("No such attribute: {}".format(name))
		return self._attributes[name]


	def readColumn2D(self, name, indices=None):
		"""
		Load a full column (all entries). Each (row x column) entry is a
		1-D NumPy array. This method can optionally read just a slice of all
		those arrays -- the subcolumns at the given `indices`.

		Parameters:
			name (str): The name of the column.
			indices (ndarray[int]): The subcolumn indices to select from each
				entry, or None to read in all data.

				If provided, this can give a performance boost for columns that
				are wide and tall.

				NOTE: The performance benefit might only be realized if the file
				is in the disk cache (i.e. the file has been recently read),
				which should typically be the case, AND only if the entries are
				large enough that reading them one at a time won't add a lot of
				I/O overhead.

		Returns:
			ndarray: a 2-D NumPy array (row x subcolumn) OR squeezed into a
			1-D array if the entries written were scalars or 1-element arrays
			or there's only one row.

		Notes:
		If entry sizes varies, this method cannot be used.

		TODO (John): Consider using np.memmap to defer loading of the data
			until it is operated on.  Early work (see issue #221) suggests that
			this may lead to cryptic performance issues.
		"""

		if name not in self._columnNames:
			raise DoesNotExistError("No such column: {}".format(name))

		entry_blocks = []

		# Read the header and read, decompress, and unpack all the blocks.
		with open(os.path.join(self._path, name), 'rb') as dataFile:
			chunk = Chunk(dataFile, align=False)
			header = _ColumnHeader(chunk)
			chunk.close()
			decompressor = (
				zlib.decompress if header.compression_type == tw.COMPRESSION_TYPE_ZLIB
				else lambda data_bytes: data_bytes)

			while True:
				try:
					chunk = Chunk(dataFile, align=False)
				except EOFError:
					break

				if chunk.getname() == tw.BLOCK_CHUNK_TYPE:
					raw = chunk.read()
					if len(raw) != chunk.getsize():
						raise EOFError('Data block cut short {}/{}'.format(
							len(raw), chunk.getsize()))

					data = decompressor(raw)
					entries = np.frombuffer(data, header.dtype).reshape(
						-1, header.elements_per_entry)
					if indices is not None:
						entries = entries[:, indices]
					entry_blocks.append(entries)

				chunk.close()  # skips the rest of the chunk including an unrecognized chunk

		# Note: frombuffer() returns a write-protected ndarray onto the
		# immutable byte str. entries[:, indices] makes a writeable array, as
		# does vstack(), otherwise this should copy the array.
		return np.vstack(entry_blocks)


	def readColumn(self, name, indices=None):
		'''
		Read a column just like readColumn2D() then squeeze() the resulting
		NumPy array, returning a 0D, 1D, or 2D array, depending on the number
		of rows and subcolumns written.

		1 row x 1 subcolumn => 0D.

		n rows x 1 subcolumn or 1 row x n subcolumns => 1D.

		n rows x m subcolumns => 2D.

		Args:
			name (str): the column name.
			indices (ndarray[int]): The subcolumn indices to select from each
				entry, or None to read in all data. See readColumn2D().

		Returns:
			A 0D, 1D, or 2D array.
		'''
		return self.readColumn2D(name, indices).squeeze()


	def allAttributeNames(self):
		"""
		Returns a list of all attribute names including Table metadata.
		"""
		return self._attributes.keys()


	def attributeNames(self):
		"""
		Returns a list of ordinary (client-provided) attribute names.
		"""
		names = [k for k in self._attributes.keys() if not k.startswith('_')]
		return names


	def columnNames(self):
		"""
		Returns the names of all columns.
		"""
		return list(self._columnNames)


	def close(self):
		"""
		Does nothing.

		The TableReader keeps no files open, so this method does nothing.

		Notes
		-----
		TODO (John): Consider removing this method.  At the moment are usage is
			inconsistent, and gives the impression that it is actually
			beneficial or necessary.
		"""
		pass
