
from __future__ import absolute_import, division, print_function

import os
import json
import numpy as np
import zlib

from wholecell.utils import filepath
from . import tablewriter as tw

__all__ = [
	"TableReader",
	]

class TableReaderError(Exception):
	"""
	Base exception class for TableReader-associated exceptions.
	"""
	pass


class VersionError(TableReaderError):
	"""
	An error raised when the input files claim to be from a different format or
	version of the file specification.
	"""
	pass


class DoesNotExistError(TableReaderError):
	"""
	An error raised when a column or attribute does not seem to exist.
	"""
	pass


class _ColumnHeader(object):
	'''Column header info read from a Column file's header chunk.'''
	def __init__(self, dataFile):
		chunk_header = dataFile.read(tw.CHUNK_HEADER.size)
		(chunk_type, chunk_size) = tw.CHUNK_HEADER.unpack(chunk_header)
		self.column_chunk_end = tw.CHUNK_HEADER.size + chunk_size

		if chunk_type != tw.COLUMN_CHUNK_TYPE:
			raise VersionError('Not a Column file or unsupported version')

		header_struct = dataFile.read(tw.COLUMN_STRUCT.size)
		(self.bytes_per_entry,
		self.elements_per_entry,
		self.entries_per_block,
		self.compression_type) = tw.COLUMN_STRUCT.unpack(header_struct)

		if self.compression_type not in (tw.COMPRESSION_TYPE_NONE, tw.COMPRESSION_TYPE_ZLIB):
			raise VersionError('Unsupported Column compression type {}'.format(
				self.compression_type))

		descr_json_len = chunk_size - tw.COLUMN_STRUCT.size
		descr_json = dataFile.read(descr_json_len)
		if len(descr_json) < descr_json_len:
			raise IOError('Column file header cut short')
		descr = json.loads(descr_json)

		if isinstance(descr, basestring):
			self.dtype = str(descr)  # really the dtype.descr
		else:
			# numpy requires list-of-tuples-of-strings
			# TODO(jerry): Support triples?
			self.dtype = [(str(n), str(t)) for n, t in descr]


class TableReader(object):
	"""
	Reads output generated by TableWriter.

	Parameters:
		path (str): Path to the input location (a directory).

	See also
	--------
	wholecell.io.tablewriter.TableWriter
	"""

	def __init__(self, path):
		self._path = path

		# Read the table's attributes file
		attributes_filename = os.path.join(path, tw.FILE_ATTRIBUTES)
		try:
			self._attributes = filepath.read_json_file(attributes_filename)

		except IOError as e:
			raise VersionError(
				"Could not read a table's attributes file ({})."
				" Version 2 tables are not supported."  # they could be...
				" Unzip all table files if needed.".format(attributes_filename), e)

		# Check if the table's version matches the expected version
		version = self._attributes['_version']
		if version != tw.VERSION:
			raise VersionError("Expected version {} but found version {}".format(
				tw.VERSION, version))

		# List the column file names. Ignore the 'attributes.json' file.
		self._columnNames = {p for p in os.listdir(path) if '.json' not in p}
		self._columnHeaders = {}  # maps column name to _ColumnHeader


	def readAttribute(self, name):
		"""
		Return an attribute value.

		Parameters:
			name (str): The attribute name.

		Returns
		-------
		The attribute value, JSON-deserialized from a string.
		"""

		if name not in self._attributes:
			raise DoesNotExistError("No such attribute: {}".format(name))
		return self._attributes[name]


	def readColumn2D(self, name, indices=None):
		"""
		Load a full column (all entries). Each (row x column) entry is a
		1-D NumPy array. This method can optionally read just a slice of all
		those arrays -- the subcolumns at the given `indices`.

		Parameters:
			name (str): The name of the column.
			indices (ndarray[int]): The subcolumn indices to select from each
				entry, or None to read in all data.

				If provided, this can give a performance boost for columns that
				are wide and tall.

				NOTE: The performance benefit might only be realized if the file
				is in the disk cache (i.e. the file has been recently read),
				which should typically be the case, AND only if the entries are
				large enough that reading them one at a time won't add a lot of
				I/O overhead.

		Returns:
			ndarray: a 2-D NumPy array (row x subcolumn) OR squeezed into a
			1-D array if the entries written were scalars or 1-element arrays
			or there's only one row.

		Notes:
		If entry sizes varies, this method cannot be used.

		TODO (John): Consider using np.memmap to defer loading of the data
			until it is operated on.  Early work (see issue #221) suggests that
			this may lead to cryptic performance issues.
		"""

		if name not in self._columnNames:
			raise DoesNotExistError("No such column: {}".format(name))

		entry_blocks = []

		with open(os.path.join(self._path, name), 'rb') as dataFile:
			header = self._loadHeader(name, dataFile)
			decompressor = (
				zlib.decompress if header.compression_type == tw.COMPRESSION_TYPE_ZLIB
				else lambda data: data)

			# Read, decompress, and unpack all the blocks.
			while True:
				block_header = dataFile.read(tw.CHUNK_HEADER.size)
				if not block_header:
					break
				if len(block_header) < tw.CHUNK_HEADER.size:
					raise EOFError('Short data block header')

				chunk_type, chunk_size = tw.CHUNK_HEADER.unpack(block_header)
				if chunk_type != tw.BLOCK_CHUNK_TYPE:
					dataFile.seek(chunk_size, os.SEEK_CUR)  # skip this unknown chunk
					continue

				raw = dataFile.read(chunk_size)
				if len(raw) != chunk_size:
					raise EOFError('Data block cut short {}/{}'.format(
						len(raw), chunk_size))
				data = decompressor(raw)
				entries = np.frombuffer(data, header.dtype).reshape(
					-1, header.elements_per_entry)
				if indices is not None:
					entries = entries[:, indices]
				entry_blocks.append(entries)

		# Note: frombuffer() returns a write-protected ndarray onto the
		# immutable byte str. entries[:, indices] makes a writeable array, as
		# does vstack(), otherwise this should copy the array.
		return np.vstack(entry_blocks)


	def readColumn(self, name, indices=None):
		'''
		Read a column just like readColumn2D() then squeeze() the resulting
		NumPy array, returning a 0D, 1D, or 2D array, depending on the number
		of rows and subcolumns written.

		1 row x 1 subcolumn => 0D.

		n rows x 1 subcolumn or 1 row x n subcolumns => 1D.

		n rows x m subcolumns => 2D.

		Args:
			name (str): the column name.
			indices (ndarray[int]): The subcolumn indices to select from each
				entry, or None to read in all data. See readColumn2D().

		Returns:
			A 0D, 1D, or 2D array.
		'''
		return self.readColumn2D(name, indices).squeeze()


	def _loadHeader(self, name, dataFile):
		"""
		Load the named column's _ColumnHeader from disk or cache.

		SIDE EFFECTS: Seeks dataFile to the end of the column header chunk.

		Parameters:
			name (str): The column name.
			dataFile (BinaryIO): The open column data file @ seek position 0.
		"""

		if name in self._columnHeaders:
			dataFile.seek(self._columnHeaders[name].column_chunk_end)
			return self._columnHeaders[name]

		header = _ColumnHeader(dataFile)
		self._columnHeaders[name] = header
		return header


	def allAttributeNames(self):
		"""
		Returns a list of all attribute names including Table metadata.
		"""
		return self._attributes.keys()


	def attributeNames(self):
		"""
		Returns a list of ordinary (client-provided) attribute names.
		"""
		names = [k for k in self._attributes.keys() if not k.startswith('_')]
		return names


	def columnNames(self):
		"""
		Returns the names of all columns.
		"""
		return list(self._columnNames)


	def close(self):
		"""
		Does nothing.

		The TableReader keeps no files open, so this method does nothing.

		Notes
		-----
		TODO (John): Consider removing this method.  At the moment are usage is
			inconsistent, and gives the impression that it is actually
			beneficial or necessary.
		"""
		pass
