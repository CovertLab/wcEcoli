
from __future__ import division
from __future__ import print_function

import os
import json

import numpy as np

from . import tablewriter as tw

ZIP_FILETYPE = ".bz2"

__all__ = [
	"TableReader",
	]

class TableReaderError(Exception):
	"""
	Base exception class for TableReader-associated exceptions.
	"""
	pass


class NotUnzippedError(TableReaderError):
	"""
	An error raised when it appears that the input files are compressed.
	"""
	pass


class VersionError(TableReaderError):
	"""
	An error raised when the input files claim to be from a different version
	of the file specification.
	"""
	pass


class DoesNotExistError(TableReaderError):
	"""
	An error raised when a column or attribute does not seem to exist.
	"""
	pass


class VariableWidthError(TableReaderError):
	"""
	An error raised when trying to load an entire column as one array when
	entry sizes vary.
	"""
	pass


class TableReader(object):
	"""
	Reads output generated by TableWriter.

	Parameters
	----------
	path : str
		Path to the input location (a directory).

	See also
	--------
	wholecell.io.tablewriter.TableWriter

	Notes
	-----
	TODO (John): Consider a method for loading an indexed portion of a column.

	TODO (John): Consider removing unused methods (see below).
	"""

	def __init__(self, path):
		# Open version file for table
		versionFilePath = os.path.join(path, tw.DIR_METADATA, tw.FILE_VERSION)
		try:
			with open(versionFilePath) as f:
				version = f.read()

		except IOError as e:
			# Check if a zipped version file exists. Print appropriate error prompts.
			if os.path.exists(versionFilePath + ZIP_FILETYPE):
				raise NotUnzippedError("The version file for a table ({}) was found zipped. Unzip all table files before reading table.".format(path), e)
			else:
				raise VersionError("Could not open the version file for a table ({})".format(path), e)

		# Check if the table version matches the latest version
		if version != tw.VERSION:
			raise VersionError("Expected version {} but found version {}".format(tw.VERSION, version))

		# Read attribute names for table
		self._dirAttributes = os.path.join(path, tw.DIR_ATTRIBUTES)
		self._attributeNames = os.listdir(self._dirAttributes)

		# Read column names for table
		self._dirColumns = os.path.join(path, tw.DIR_COLUMNS)
		self._columnNames = os.listdir(self._dirColumns)


	def readAttribute(self, name):
		"""
		Load an attribute.

		Parameters
		----------
		name : str
			The name of the attribute.

		Returns
		-------
		The attribute, JSON-deserialized from a string.

		"""

		if name not in self._attributeNames:
			raise DoesNotExistError("No such attribute: {}".format(name))

		return json.loads(
			open(os.path.join(self._dirAttributes, name)).read()
			)


	def readColumn(self, name, indices=None):
		"""
		Load a full column (all entries). Each (row x column) entry is a
		1-D NumPy array. This method can optionally read just a slice of all
		those arrays: their subcolumns at the given `indices`.

		Parameters:
			name (str): The name of the column.
			indices (ndarray): Numpy array of ints. The specific subcolumn
				indices to read from each entry. If None, reads in all data.
				If provided, can give a performance boost for columns with many
				entries.
				NOTE: The performance benefit might only be realized if the file
				is in the disk cache (i.e. the file has been recently read),
				which should typically be the case.

		Returns:
			ndarray: a 2-D NumPy array (row x subcolumn) OR squeezed into a
			1-D array if the entries written were scalars or 1-element arrays
			or there's only one row.

		Notes:
		If entry sizes varies, this method cannot be used.

		TODO (John): Consider using np.memmap to defer loading of the data
			until it is operated on.  Early work (see issue #221) suggests that
			this may lead to cryptic performance issues.
		"""

		if name not in self._columnNames:
			raise DoesNotExistError("No such column: {}".format(name))

		offsets, dtype = self._loadOffsets(name)

		sizes = np.diff(offsets)

		if len(set(sizes)) > 1:
			raise VariableWidthError("Cannot load full column; data size varies")

		entry_size = sizes[0]
		nEntries = sizes.size

		with open(os.path.join(self._dirColumns, name, tw.FILE_DATA), 'rb') as dataFile:
			if indices is None:
				dataFile.seek(offsets[0])

				return np.frombuffer(
					dataFile.read(), dtype
					).copy().reshape(nEntries, -1).squeeze()
			else:
				data = np.zeros((nEntries, len(indices)), dtype)

				dataFile.seek(offsets[0])

				for i in range(nEntries):
					data[i, :] = np.frombuffer(
						dataFile.read(entry_size), dtype
						)[indices]

				return data.squeeze()


	def _loadOffsets(self, name):
		"""
		Internal method for loading data needed to interpret a column.

		Parameters
		----------
		name : str
			The name of the column.

		Returns
		-------
		offsets : ndarray (int)
			An integer array.  Each element corresponds to the offset (in
			bytes) for each entry in the column's data file.
		dtype : list-of-tuples-of-strings
			A data type specific for instantiating a NumPy ndarray.

		"""

		with open(os.path.join(self._dirColumns, name, tw.FILE_OFFSETS)) as offsetsFile:
			offsets = np.array([int(i.strip()) for i in offsetsFile])

		with open(os.path.join(self._dirColumns, name, tw.FILE_DATA), 'rb') as dataFile:
			rawDtype = json.loads(dataFile.read(offsets[0]).decode('utf-8'))

			if isinstance(rawDtype, basestring):
				dtype = str(rawDtype)

			else:
				dtype = [ # numpy requires list-of-tuples-of-strings
					(str(n), str(t))
					for n, t in rawDtype
					]

		return offsets, dtype


	def attributeNames(self):
		"""
		Returns the names of all attributes.
		"""
		return self._attributeNames


	def columnNames(self):
		"""
		Returns the names of all columns.
		"""
		return self._columnNames


	def close(self):
		"""
		Does nothing.

		The TableReader keeps no files open, so this method does nothing.

		Notes
		-----
		TODO (John): Consider removing this method.  At the moment are usage is
			inconsistent, and gives the impression that it is actually
			beneficial or necessary.

		"""
		pass
